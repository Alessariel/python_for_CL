{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коллокации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллокации - это устойчивые выражения, состоящие из двух и более слов. Устойчивые - значит, что они часто используются вместе.\n",
    "Сегодня мы попробуем различные способы извлечения коллокаций из текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 0 : кастомные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: razdel in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install razdel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[что такое razdel?](https://pypi.org/project/razdel/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# традиционная ячейка импортов\n",
    "import itertools\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "# раскомментьте строчки ниже, если NLTK вызывает проблемы при импорте\n",
    "\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian') + [\"это\", \"весь\"]) # определили список стоп-слов\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# напишем несколько функций для предобработки текста\n",
    "\n",
    "# нормализация текста\n",
    "def normalize(text):\n",
    "    \n",
    "    tokens = re.findall('[а-яёa-z0-9]+', text.lower())\n",
    "    normalized_text = [morph.parse(word)[0].normal_form for word \\\n",
    "                                                            in tokens]\n",
    "    \n",
    "    normalized_text = [word for word in normalized_text if len(word) > 2 \\\n",
    "                       and word not in stops]\n",
    "    \n",
    "    return normalized_text\n",
    "\n",
    "# обработка текста\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text) # разбили текст на предложения\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "# возвращаем список нормализованных предложений\n",
    "\n",
    "def ngrammer(tokens, stops, n=2):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens if token not in stops]\n",
    "    # добавляем все слова текста кроме стоп-слов\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        # для каждого элемента списка, включительно, минус окно:\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "        # присоединяем элемент и все соседние в зависимости от размера окна\n",
    "    return ngrams # возвращаем список таких кортежей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем почти также, только теперь нам не нужны тэги начала и конца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "попробуем на каком-нибудь тестовом тексте, например, [про корги](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BB%D1%8C%D1%88-%D0%BA%D0%BE%D1%80%D0%B3%D0%B8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = open('corgi.txt', encoding='utf-8').read()\n",
    "# измените путь, если ваш датасет лежит в другом месте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь предобработаем наш корпус заранее написанной функцией *(может занять некоторое время)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus -- это список списков. Давайте на него посмотрим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['контрастный',\n",
       "  'обводка',\n",
       "  'пасти',\n",
       "  'многий',\n",
       "  'казаться',\n",
       "  'пемброк',\n",
       "  'улыбаться'],\n",
       " ['глаз', 'карий'],\n",
       " ['красивый',\n",
       "  'крепкий',\n",
       "  'корпус',\n",
       "  'крепкий',\n",
       "  'короткий',\n",
       "  'лапа',\n",
       "  'выраженный',\n",
       "  'угол'],\n",
       " ['считаться',\n",
       "  'пемброк',\n",
       "  'отличаться',\n",
       "  'кардиган',\n",
       "  'отсутствие',\n",
       "  'хвост',\n",
       "  'однако',\n",
       "  'ошибка'],\n",
       " ['пемброк', 'рождаться', 'куцехвостый', 'хвост', 'купировать']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "\n",
    "display(corpus[30:35]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке много всяких чисел, однобуквеных слов и стоп-слов.  Добавим какие-нибудь ограничения к коду выше, чтобы биграммы получались почище."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "\n",
    "for sent in corpus:\n",
    "    word_counter.update(ngrammer(sent, n=2, stops=stops))\n",
    "    \n",
    "#метод .update() (похож на традиционный .update() у словарей) обновляет показатели счетчика, а значит, и частоты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "подробнее про Counter: [небольшой тьюториал](https://pythonworld.ru/moduli/modul-collections.html) и [официальная документация](https://docs.python.org/3/library/collections.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('вельша', 'корги'), 20),\n",
       " (('корги', 'пемброк'), 8),\n",
       " (('корги', 'относиться'), 3),\n",
       " (('корги', 'кардиган'), 3),\n",
       " (('пемброк', 'кардиган'), 3),\n",
       " (('рыжий', 'белый'), 3),\n",
       " (('пастуший', 'собака'), 2),\n",
       " (('англ', 'welsh'), 2),\n",
       " (('welsh', 'corgi'), 2),\n",
       " (('получить', 'распространение'), 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке есть коллокации, которые попали туда, т.к. одно слово встречается в разных контекстах. \n",
    "\n",
    "Однако нас интересуют случаи, когда одни и те же слова встречаются вместе. Для этого мы можем придумать какие-нибудь формулы, учитывающие частоты слов по отдельности и общую частоту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ - взять количество упоминаний какой-то биграммы и поделить на сумму количеств упоминаний слов по отдельности. \n",
    "\n",
    "$\\frac{частота (биграмма)}{частота(слово1)+частота(слово2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая формула называется PMI ([здесь подробнее](https://en.wikipedia.org/wiki/Pointwise_mutual_information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем свою формулу подсчета pmi\n",
    "def scorer_simple(word_count_a, word_count_b, bigram_count, *args):\n",
    "    try:\n",
    "        score = bigram_count/((word_count_a+word_count_b))\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем функцию, которая будет делать счетчики для слов и биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_stats(corpus, stops):\n",
    "    ## соберем статистики для отдельных слов и биграмм\n",
    "    \n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    \n",
    "    for sent in corpus:\n",
    "        unigrams.update(sent)\n",
    "        bigrams.update(ngrammer(sent, stops, 2))\n",
    "    \n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И функцию, которая пройдет по всем биграммам и вычислит для них нашу метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bigrams(unigrams, bigrams, scorer, threshold=-100000, min_count=1):\n",
    "    # посчитаем метрику для каждого нграмма\n",
    "    \n",
    "    bigram2score = Counter()\n",
    "    len_vocab = len(unigrams)\n",
    "    for bigram in bigrams:\n",
    "        score = scorer(unigrams[bigram[0]], unigrams[bigram[1]], \n",
    "                       bigrams[bigram], len_vocab, min_count)\n",
    "        \n",
    "        # если метрика выше порога, добавляем в словарик\n",
    "        if score > threshold:\n",
    "            bigram2score[bigram] = score\n",
    "    \n",
    "    return bigram2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = collect_stats(corpus, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема с таким подходом в том, что на самом верху окажутся слова, которые встречают по одному разу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('англ', 'welsh'), 0.5),\n",
       " (('наиболее', 'популярный'), 0.5),\n",
       " (('завезти', 'кельт'), 0.5),\n",
       " (('кельт', 'освоение'), 0.5),\n",
       " (('вывести', 'пембрукшир'), 0.5),\n",
       " (('пембрукшир', 'предположительно'), 0.5),\n",
       " (('история', 'xiii'), 0.5),\n",
       " (('достоверно', 'неизвестно'), 0.5),\n",
       " (('теория', 'счёт'), 0.5),\n",
       " (('cur', 'смотреть'), 0.5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому можно немного переделать оценивающую функцию, добавив минимальное число вхождений для биграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(word_count_a, word_count_b, bigram_count, len_vocab, min_count):\n",
    "    try:\n",
    "        score = ((bigram_count - min_count) / ((word_count_a + word_count_b)))\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2score = score_bigrams(unigrams, bigrams, scorer, min_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('вельша', 'корги'), 0.0),\n",
       " (('корги', 'пемброк'), -0.23076923076923078),\n",
       " (('порода', 'корги'), -0.3877551020408163),\n",
       " (('корги', 'кардиган'), -0.40476190476190477),\n",
       " (('собака', 'вельша'), -0.4222222222222222),\n",
       " (('собака', 'порода'), -0.43902439024390244),\n",
       " (('корги', 'относиться'), -0.4594594594594595),\n",
       " (('порода', 'собака'), -0.4634146341463415),\n",
       " (('год', 'корги'), -0.475),\n",
       " (('корги', 'свой'), -0.475)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram2score.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*А [по этой ссылке](http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-55462016000300327#t1) можно прочитать про другие метрики*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: Все готовое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Писать все это самому конечно не обязательно. Попробуем использовать уже существующие специальные функции из библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Удобно пользоваться [phraser из gensim'а](https://radimrehurek.com/gensim/models/phrases.html). Он собирает статистику по корпусу, а затем склеивает слова в биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем статистики\n",
    "ph = gensim.models.Phrases(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразовывать можно и через ph, но так быстрее \n",
    "p = gensim.models.phrases.Phraser(ph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию там используется метрики из статьи про word2vec и ещё есть нормализованные pmi.\n",
    "Если не нравятся функции оценки, то ему можно подать любую другую функцию. Интерфейс у функции там почти точно такой же как и у наших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем статистики по уже забиграммленному тексту\n",
    "ph2 = gensim.models.Phrases(p[corpus])\n",
    "p2 = gensim.models.phrases.Phraser(ph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['контрастный',\n",
       " 'обводка',\n",
       " 'пасти',\n",
       " 'многий',\n",
       " 'казаться',\n",
       " 'пемброк',\n",
       " 'улыбаться']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2[p[corpus[30]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "Тут больше метрик, но преборазователь слов в нграммы нужно написать самому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import * # звездочка говорит, что мы импортируем всё"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "из NLTK будем использовать модуль с коллокациями, [вот подробнее про него](https://www.nltk.org/howto/collocations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() \n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder2 = BigramCollocationFinder.from_documents(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder3 = TrigramCollocationFinder.from_documents(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('вельша', 'корги'),\n",
       " ('рыжий', 'белый'),\n",
       " ('корги', 'пемброк'),\n",
       " ('англ', 'welsh'),\n",
       " ('будущий', 'король'),\n",
       " ('герцог', 'йоркский'),\n",
       " ('дочь', 'элизабет'),\n",
       " ('грудной', 'клетка'),\n",
       " ('получить', 'распространение'),\n",
       " ('welsh', 'corgi'),\n",
       " ('тигровый', 'окрас'),\n",
       " ('1933', 'год'),\n",
       " ('свой', 'дочь'),\n",
       " ('королевский', 'семья'),\n",
       " ('британский', 'королевский'),\n",
       " ('короткий', 'лапа'),\n",
       " ('crufts', 'кинологический'),\n",
       " ('cur', 'смотреть'),\n",
       " ('eagle', 'коротко'),\n",
       " ('golden', 'eagle')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder2.nbest(bigram_measures.likelihood_ratio, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crufts', 'кинологический', 'мероприятие'),\n",
       " ('cur', 'смотреть', 'сторожить'),\n",
       " ('golden', 'eagle', 'коротко'),\n",
       " ('rozavel', 'golden', 'eagle'),\n",
       " ('аджилить', 'флайбола', 'соревнование'),\n",
       " ('актёр', 'дэниэлом', 'крейг'),\n",
       " ('благородный', 'очертание', 'прямой'),\n",
       " ('благородство', 'мощь', 'работоспособность'),\n",
       " ('важно', 'видеть', 'довольный'),\n",
       " ('вальхунд', 'вестготашпиц', 'исландский'),\n",
       " ('ввести', 'запрет', 'купирование'),\n",
       " ('величество', 'монтить', 'уиллоу'),\n",
       " ('вероятно', 'послужить', 'шведский'),\n",
       " ('видеть', 'довольный', 'пёс'),\n",
       " ('вместе', 'хозяйка', 'актёр'),\n",
       " ('возбудимый', 'живой', 'чуткий'),\n",
       " ('возникать', 'проблема', 'здоровье'),\n",
       " ('вывести', 'пембрукшир', 'предположительно'),\n",
       " ('голос', 'увидеть', 'знакомый'),\n",
       " ('двор', 'жить', 'поколение')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder3.nbest(trigram_measures.pmi, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn\n",
    "\n",
    "Sklearn напрямую не предназначен для этого, но из него тоже можно вытаскивать устойчивые нграммы. Tfidf подходит как метрика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот рассказ о том, [что такое TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# по умолчанию векторайзер сам токенизирует, поэтому проще склеить токены через пробел\n",
    "texts = [' '.join(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.7, max_features=100,\n",
    "                       ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.7, max_features=100, min_df=2, ngram_range=(2, 2))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'пастуший собака': 16,\n",
       " 'корги относиться': 11,\n",
       " 'вельша корги': 6,\n",
       " 'корги кардиган': 10,\n",
       " 'англ welsh': 2,\n",
       " 'welsh corgi': 1,\n",
       " 'корги пемброк': 12,\n",
       " 'получить распространение': 20,\n",
       " '1933 год': 0,\n",
       " 'герцог йоркский': 7,\n",
       " 'будущий король': 5,\n",
       " 'щенок вельша': 26,\n",
       " 'свой дочь': 22,\n",
       " 'дочь элизабет': 9,\n",
       " 'пемброк кардиган': 17,\n",
       " 'тигровый окрас': 25,\n",
       " 'грудной клетка': 8,\n",
       " 'пемброк рыжий': 19,\n",
       " 'рыжий белый': 21,\n",
       " 'белый окрас': 3,\n",
       " 'короткий лапа': 15,\n",
       " 'свой хозяин': 23,\n",
       " 'корги склонный': 13,\n",
       " 'пемброк кличка': 18,\n",
       " 'британский королевский': 4,\n",
       " 'королевский семья': 14,\n",
       " 'собака порода': 24}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# словарь со словами и индекасами\n",
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.32022832, 4.32022832, 4.32022832, 4.32022832, 4.32022832,\n",
       "       4.32022832, 2.37431817, 4.32022832, 4.32022832, 4.32022832,\n",
       "       4.03254625, 4.03254625, 3.22161603, 4.32022832, 4.32022832,\n",
       "       4.32022832, 4.32022832, 4.03254625, 4.32022832, 4.32022832,\n",
       "       4.32022832, 4.03254625, 4.32022832, 4.32022832, 4.32022832,\n",
       "       4.32022832, 4.32022832])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# массив с метриками, можно достать по индексу из словаря\n",
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idf = []\n",
    "\n",
    "for word, i in tfidf.vocabulary_.items():\n",
    "    word2idf.append((tfidf.idf_[i], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.320228319128488, 'щенок вельша'),\n",
       " (4.320228319128488, 'тигровый окрас'),\n",
       " (4.320228319128488, 'собака порода'),\n",
       " (4.320228319128488, 'свой хозяин'),\n",
       " (4.320228319128488, 'свой дочь'),\n",
       " (4.320228319128488, 'получить распространение'),\n",
       " (4.320228319128488, 'пемброк рыжий'),\n",
       " (4.320228319128488, 'пемброк кличка'),\n",
       " (4.320228319128488, 'пастуший собака'),\n",
       " (4.320228319128488, 'короткий лапа'),\n",
       " (4.320228319128488, 'королевский семья'),\n",
       " (4.320228319128488, 'корги склонный'),\n",
       " (4.320228319128488, 'дочь элизабет'),\n",
       " (4.320228319128488, 'грудной клетка'),\n",
       " (4.320228319128488, 'герцог йоркский'),\n",
       " (4.320228319128488, 'будущий король'),\n",
       " (4.320228319128488, 'британский королевский'),\n",
       " (4.320228319128488, 'белый окрас'),\n",
       " (4.320228319128488, 'англ welsh'),\n",
       " (4.320228319128488, 'welsh corgi'),\n",
       " (4.320228319128488, '1933 год'),\n",
       " (4.0325462466767075, 'рыжий белый'),\n",
       " (4.0325462466767075, 'пемброк кардиган'),\n",
       " (4.0325462466767075, 'корги относиться'),\n",
       " (4.0325462466767075, 'корги кардиган'),\n",
       " (3.2216160304603783, 'корги пемброк'),\n",
       " (2.374318170073175, 'вельша корги')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word2idf, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразовать отдельный текст в список нграммов сложнее, но можно (правда порядок не получится сохранить). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
